{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertModel, BertTokenizer\r\n",
    "from transformers import AdamW\r\n",
    "from transformers import get_linear_schedule_with_warmup\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "from .data.squad_dataloader import SquadDataset\r\n",
    "from .data.data_utils import squad_collate_fn\r\n",
    "from .run_fine_tuning.squad_args import squad_args\r\n",
    "from .run_fine_tuning.run_train_squad import train_epoch\r\n",
    "from .run_fine_tuning.run_valid_squad import eval_epoch\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "import pandas as pd\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "\r\n",
    "args = squad_args()\r\n",
    "d_hidden = args.d_hidden\r\n",
    "device = args.device\r\n",
    "batch_size = args.batch_size\r\n",
    "n_epoch = args.n_epoch\r\n",
    "learning_rate = args.learning_rate\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#Load train and test dataset\r\n",
    "train_dataset = SquadDataset('./SQuAD_train.json')\r\n",
    "train_loader = DataLoader(\r\n",
    "    train_dataset, \r\n",
    "    batch_size= batch_size, \r\n",
    "    shuffle=True, \r\n",
    "    collate_fn=squad_collate_fn, \r\n",
    "    drop_last= True\r\n",
    "    )\r\n",
    "\r\n",
    "valid_dataset = SquadDataset('./SQuAD_test.json')\r\n",
    "valid_loader = DataLoader(\r\n",
    "    valid_dataset,\r\n",
    "    batch_size= batch_size, \r\n",
    "    shuffle=True, \r\n",
    "    collate_fn=squad_collate_fn, \r\n",
    "    drop_last= True\r\n",
    "    )\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Load tokenizer\r\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
    "\r\n",
    "#Load model\r\n",
    "class QuestionAnswering(nn.Module):\r\n",
    "    def __init__(self, d_hidden):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\r\n",
    "        self.projection = nn.Linear(d_hidden, 2)\r\n",
    "\r\n",
    "    def get_token_emb(self):\r\n",
    "        return self.model.get_input_embeddings()\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        bert_outputs = self.model(inputs)[0]\r\n",
    "        pred_scores = self.projection(bert_outputs)\r\n",
    "        return pred_scores\r\n",
    "\r\n",
    "model = QuestionAnswering(device, d_hidden)\r\n",
    "model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#Criterion, Optimizer and Scheduler\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "optimizer = AdamW(\r\n",
    "    model.parameters(), \r\n",
    "    lr = learning_rate, \r\n",
    "    correct_bias = False\r\n",
    "    ) #default lr  = 5e-5, eps = 1e-8\r\n",
    "total_steps = len(train_loader) * n_epoch\r\n",
    "scheduler = get_linear_schedule_with_warmup(\r\n",
    "    optimizer, \r\n",
    "    num_warmup_steps = 0, \r\n",
    "    num_training_steps = total_steps\r\n",
    "    )\r\n",
    "\r\n",
    "#random seed\r\n",
    "seed_val = 100\r\n",
    "random.seed(seed_val)\r\n",
    "np.random.seed(seed_val)\r\n",
    "torch.manual_seed(seed_val)\r\n",
    "torch.cuda.manual_seed_all(seed_val)\r\n",
    "\r\n",
    "#gradient init\r\n",
    "model.zero_grad()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "losses = []\r\n",
    "scores = []\r\n",
    "\r\n",
    "for epoch in range(n_epoch):\r\n",
    "    #train_epoch\r\n",
    "    model.train()\r\n",
    "    loss_epoch = []\r\n",
    "\r\n",
    "    with tqdm(total = len(train_loader), desc = f\"Train {epoch}\") as pbar:\r\n",
    "        for i, value in enumerate(train_loader):\r\n",
    "            _, answer_indices, bert_inputs = map(lambda x:x.to(device), value)\r\n",
    "\r\n",
    "            loss = train_epoch(\r\n",
    "                model, \r\n",
    "                criterion, \r\n",
    "                optimizer, \r\n",
    "                answer_indices,\r\n",
    "                bert_inputs\r\n",
    "                )\r\n",
    "            loss_epoch.append(loss)\r\n",
    "\r\n",
    "            scheduler.step()\r\n",
    "\r\n",
    "            pbar.update(1)\r\n",
    "            pbar.set_postfix_str(f\"Loss: {loss:.3f} ({np.mean(loss_epoch):.3f})\")\r\n",
    "\r\n",
    "    losses.append(np.mean(loss_epoch))\r\n",
    "\r\n",
    "    #valid_epoch\r\n",
    "    model.eval()\r\n",
    "    score_epoch = []\r\n",
    "\r\n",
    "    with tqdm(total = len(valid_loader), desc = f\"Valid {epoch}\") as pbar:\r\n",
    "        for i, value in enumerate(valid_loader): \r\n",
    "            answers, _, bert_inputs = map(lambda x:x.to(device), value)\r\n",
    "\r\n",
    "            score = eval_epoch(\r\n",
    "                model, \r\n",
    "                answers, \r\n",
    "                bert_inputs, \r\n",
    "                batch_size, \r\n",
    "                tokenizer\r\n",
    "                )\r\n",
    "            score_epoch.append(score)\r\n",
    "\r\n",
    "            pbar.update(1)\r\n",
    "            pbar.set_postfix_str(f\"Acc: {score:.3f} ({np.mean(score_epoch):.3f})\")\r\n",
    "\r\n",
    "    scores.append(np.mean(score_epoch))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#loss and score graph\r\n",
    "losses = [1.0, 2.0, 3.0]\r\n",
    "scores = [3.1, 2.1, 5.7]\r\n",
    "data = {\r\n",
    "    \"loss\" : losses,\r\n",
    "    \"score\" : scores\r\n",
    "}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "display(df)\r\n",
    "\r\n",
    "plt.figure(figsize = [12, 4])\r\n",
    "plt.plot(losses, label=\"loss\")\r\n",
    "plt.legend()\r\n",
    "plt.xlabel('Epoch')\r\n",
    "plt.ylabel('Value')\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "   loss  score\n",
       "0   1.0    3.1\n",
       "1   2.0    2.1\n",
       "2   3.0    5.7"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18180/2924672462.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "830fc09f41f4a8127b783ad2119e0f8b44080ccbdb0580da15d6f44fb7345381"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}