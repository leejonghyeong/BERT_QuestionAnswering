{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertModel, BertTokenizer\r\n",
    "from transformers import AdamW\r\n",
    "from transformers import get_linear_schedule_with_warmup\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "from .data.squad_dataloader import SquadDataset\r\n",
    "from .data.data_utils import squad_collate_fn\r\n",
    "from .run_fine_tuning.squad_args import squad_args\r\n",
    "from .run_fine_tuning.run_train_squad import train_epoch\r\n",
    "from .run_fine_tuning.run_valid_squad import eval_epoch\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "import pandas as pd\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "\r\n",
    "args = squad_args()\r\n",
    "d_hidden = args.d_hidden\r\n",
    "device = args.device\r\n",
    "batch_size = args.batch_size\r\n",
    "n_epoch = args.n_epoch\r\n",
    "learning_rate = args.learning_rate\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#Load train and test dataset\r\n",
    "train_dataset = SquadDataset('./SQuAD_train.json')\r\n",
    "train_loader = DataLoader(train_dataset, batch_size= batch_size, shuffle=True, collate_fn=squad_collate_fn, drop_last= True)\r\n",
    "\r\n",
    "valid_dataset = SquadDataset('./SQuAD_test.json')\r\n",
    "valid_loader = DataLoader(valid_dataset, batch_size= batch_size, shuffle=True, collate_fn=squad_collate_fn, drop_last= True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Load tokenizer\r\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
    "\r\n",
    "#Load model\r\n",
    "class QuestionAnswering(nn.Module):\r\n",
    "    def __init__(self, d_hidden):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\r\n",
    "        self.projection = nn.Linear(d_hidden, 2)\r\n",
    "\r\n",
    "    def get_token_emb(self):\r\n",
    "        return self.model.get_input_embeddings()\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        bert_outputs = self.model(inputs)[0]\r\n",
    "        pred_scores = self.projection(bert_outputs)\r\n",
    "        return pred_scores\r\n",
    "\r\n",
    "model = QuestionAnswering(device, d_hidden)\r\n",
    "model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#Criterion, Optimizer and Scheduler\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "optimizer = AdamW(model.parameters(), lr = learning_rate, correct_bias = False) #default lr  = 5e-5, eps = 1e-8\r\n",
    "total_steps = len(train_loader) * n_epoch\r\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\r\n",
    "\r\n",
    "#random seed\r\n",
    "seed_val = 100\r\n",
    "random.seed(seed_val)\r\n",
    "np.random.seed(seed_val)\r\n",
    "torch.manual_seed(seed_val)\r\n",
    "torch.cuda.manual_seed_all(seed_val)\r\n",
    "\r\n",
    "#gradient init\r\n",
    "model.zero_grad()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "losses = []\r\n",
    "scores = []\r\n",
    "\r\n",
    "for epoch in range(n_epoch):\r\n",
    "    #train_epoch\r\n",
    "    model.train()\r\n",
    "    loss_epoch = []\r\n",
    "\r\n",
    "    with tqdm(total = len(train_loader), desc = f\"Train {epoch}\") as pbar:\r\n",
    "        for i, value in enumerate(train_loader):\r\n",
    "            _, answer_indices, bert_inputs = map(lambda x:x.to(device), value)\r\n",
    "\r\n",
    "            loss = train_epoch(model, criterion, optimizer, answer_indices, bert_inputs)\r\n",
    "            loss_epoch.append(loss)\r\n",
    "\r\n",
    "            scheduler.step()\r\n",
    "\r\n",
    "            pbar.update(1)\r\n",
    "            pbar.set_postfix_str(f\"Loss: {loss:.3f} ({np.mean(loss_epoch):.3f})\")\r\n",
    "\r\n",
    "    losses.append(np.mean(loss_epoch))\r\n",
    "\r\n",
    "    #valid_epoch\r\n",
    "    model.eval()\r\n",
    "    score_epoch = []\r\n",
    "\r\n",
    "    with tqdm(total = len(valid_loader), desc = f\"Valid {epoch}\") as pbar:\r\n",
    "        for i, value in enumerate(valid_loader): \r\n",
    "            answers, _, bert_inputs = map(lambda x:x.to(device), value)\r\n",
    "\r\n",
    "            score = eval_epoch(model, answers, bert_inputs, batch_size, tokenizer)\r\n",
    "            score_epoch.append(score)\r\n",
    "\r\n",
    "            pbar.update(1)\r\n",
    "            pbar.set_postfix_str(f\"Acc: {score:.3f} ({np.mean(score_epoch):.3f})\")\r\n",
    "\r\n",
    "    scores.append(np.mean(score_epoch))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#loss and score graph\r\n",
    "data = {\r\n",
    "    \"loss\" : losses,\r\n",
    "    \"score\" : scores\r\n",
    "}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "display(df)\r\n",
    "\r\n",
    "plt.figure(figsize = [12, 4])\r\n",
    "plt.plot(losses, label=\"loss\")\r\n",
    "plt.legend()\r\n",
    "plt.xlabel('Epoch')\r\n",
    "plt.ylabel('Value')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "830fc09f41f4a8127b783ad2119e0f8b44080ccbdb0580da15d6f44fb7345381"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}